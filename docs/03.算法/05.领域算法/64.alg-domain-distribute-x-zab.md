---
title: 分布式算法 - ZAB算法
date: 2024-05-27 18:12:32
permalink: /pages/alg-domain-distribute-x-zab/
categories:
  - 算法
tags:
  - 
author: 
  name: Cyan
  link: https://github.com/willpast
---
# 分布式算法 - ZAB算法

> ZAB 协议全称：Zookeeper Atomic Broadcast（Zookeeper 原子广播协议）,
> 它应该是所有一致性协议中生产环境中应用最多的了。为什么呢？因为它是为 Zookeeper 设计的分布式一致性协议！

 

## 什么是 ZAB 协议？ ZAB 协议介绍

> ZAB 协议全称：Zookeeper Atomic Broadcast（Zookeeper 原子广播协议）。

  1. Zookeeper 是一个为分布式应用提供高效且可靠的分布式协调服务。在解决分布式一致性方面，Zookeeper 并没有使用 Paxos ，而是采用了 ZAB 协议。

  2. ZAB 协议定义：**ZAB 协议是为分布式协调服务 Zookeeper 专门设计的一种支持`崩溃恢复` 和 `原子广播` 协议**。下面我们会重点讲这两个东西。

  3. 基于该协议，Zookeeper 实现了一种 主备模式 的系统架构来保持集群中各个副本之间数据一致性。具体如下图所示：

![img](https://cdn.jsdelivr.net/gh/willpast/image/blog/ka_java/alg-zab-1.png)

上图显示了 Zookeeper 如何处理集群中的数据。所有客户端写入数据都是写入到 主进程（称为 Leader）中，然后，由 Leader
复制到备份进程（称为 Follower）中。从而保证数据一致性。从设计上看，和 Raft 类似。

  4. 那么复制过程又是如何的呢？复制过程类似 2PC，ZAB 只需要 Follower 有一半以上返回 Ack 信息就可以执行提交，大大减小了同步阻塞。也提高了可用性。

简单介绍完，开始重点介绍 `消息广播` 和 `崩溃恢复`。**整个 Zookeeper 就是在这两个模式之间切换** 。 简而言之，当 Leader
服务可以正常使用，就进入消息广播模式，当 Leader 不可用时，则进入崩溃恢复模式。

## 消息广播

ZAB 协议的消息广播过程使用的是一个原子广播协议，类似一个 二阶段提交过程。对于客户端发送的写请求，全部由 Leader 接收，Leader
将请求封装成一个事务 Proposal，将其发送给所有 Follwer ，然后，根据所有 Follwer 的反馈，如果超过半数成功响应，则执行 commit
操作（先提交自己，再发送 commit 给所有 Follwer）。

基本上，整个广播流程分为 3 步骤：

1.将数据都复制到 Follwer 中

![img](https://cdn.jsdelivr.net/gh/willpast/image/blog/ka_java/alg-zab-2.png)

等待 Follwer 回应 Ack，最低超过半数即成功

![img](https://cdn.jsdelivr.net/gh/willpast/image/blog/ka_java/alg-zab-3.png)

当超过半数成功回应，则执行 commit ，同时提交自己

![img](https://cdn.jsdelivr.net/gh/willpast/image/blog/ka_java/alg-zab-4.png)

通过以上 3 个步骤，就能够保持集群之间数据的一致性。实际上，在 Leader 和 Follwer
之间还有一个消息队列，用来解耦他们之间的耦合，避免同步，实现异步解耦。

还有一些细节：

  * Leader 在收到客户端请求之后，会将这个请求封装成一个事务，并给这个事务分配一个全局递增的唯一 ID，称为事务ID（ZXID），ZAB 兮协议需要保证事务的顺序，因此必须将每一个事务按照 ZXID 进行先后排序然后处理。
  * 在 Leader 和 Follwer 之间还有一个消息队列，用来解耦他们之间的耦合，解除同步阻塞。
  * zookeeper集群中为保证任何所有进程能够有序的顺序执行，只能是 Leader 服务器接受写请求，即使是 Follower 服务器接受到客户端的请求，也会转发到 Leader 服务器进行处理。
  * 实际上，这是一种简化版本的 2PC，不能解决单点问题。等会我们会讲述 ZAB 如何解决单点问题（即 Leader 崩溃问题）。

## 崩溃恢复

刚刚我们说消息广播过程中，Leader 崩溃怎么办？还能保证数据一致吗？如果 Leader 先本地提交了，然后 commit 请求没有发送出去，怎么办？

实际上，当 Leader 崩溃，即进入我们开头所说的崩溃恢复模式（崩溃即：Leader 失去与过半 Follwer 的联系）。下面来详细讲述。

  * 假设1：Leader 在复制数据给所有 Follwer 之后崩溃，怎么办？
  * 假设2：Leader 在收到 Ack 并提交了自己，同时发送了部分 commit 出去之后崩溃怎么办？

针对这些问题，ZAB 定义了 2 个原则：

  * ZAB 协议确保那些已经在 Leader 提交的事务最终会被所有服务器提交。
  * ZAB 协议确保丢弃那些只在 Leader 提出/复制，但没有提交的事务。

所以，ZAB 设计了下面这样一个选举算法：**能够确保提交已经被 Leader 提交的事务，同时丢弃已经被跳过的事务** 。

针对这个要求，如果让 Leader 选举算法能够保证新选举出来的 Leader 服务器拥有集群总所有机器编号（即 ZXID
最大）的事务，那么就能够保证这个新选举出来的 Leader 一定具有所有已经提交的提案。

而且这么做有一个好处是：**可以省去 Leader 服务器检查事务的提交和丢弃工作的这一步操作** 。

![img](https://cdn.jsdelivr.net/gh/willpast/image/blog/ka_java/alg-zab-5.png)

这样，我们刚刚假设的两个问题便能够解决。假设 1 最终会丢弃调用没有提交的数据，假设 2 最终会同步所有服务器的数据。这个时候，就引出了一个问题，如何同步？

## 数据同步

当崩溃恢复之后，需要在正式工作之前（接收客户端请求），Leader 服务器首先确认事务是否都已经被过半的 Follwer
提交了，即是否完成了数据同步。目的是为了保持数据一致。

当所有的 Follwer 服务器都成功同步之后，Leader 会将这些服务器加入到可用服务器列表中。

实际上，Leader 服务器处理或丢弃事务都是依赖着 ZXID 的，那么这个 ZXID 如何生成呢？

答：在 ZAB 协议的事务编号 ZXID 设计中，ZXID 是一个 64 位的数字，其中低 32
位可以看作是一个简单的递增的计数器，针对客户端的每一个事务请求，Leader 都会产生一个新的事务 Proposal 并对该计数器进行 + 1 操作。

而高 32 位则代表了 Leader 服务器上取出本地日志中最大事务 Proposal 的 ZXID，并从该 ZXID 中解析出对应的 epoch
值，然后再对这个值加一。

![img](https://cdn.jsdelivr.net/gh/willpast/image/blog/ka_java/alg-zab-6.png)

高 32 位代表了每代 Leader 的唯一性，低 32 代表了每代 Leader 中事务的唯一性。同时，也能让 Follwer 通过高 32 位识别不同的
Leader。简化了数据恢复流程。

基于这样的策略：当 Follower 链接上 Leader 之后，Leader 服务器会根据自己服务器上最后被提交的 ZXID 和 Follower 上的
ZXID 进行比对，比对结果要么回滚，要么和 Leader 同步。

## 总结

ZAB 协议和我们之前看的 Raft 协议实际上是有相似之处的，比如都有一个 Leader，用来保证一致性（Paxos 并没有使用 Leader
机制保证一致性）。再有采取过半即成功的机制保证服务可用（实际上 Paxos 和 Raft 都是这么做的）。

ZAB 让整个 Zookeeper 集群在两个模式之间转换，消息广播和崩溃恢复，消息广播可以说是一个简化版本的 2PC，通过崩溃恢复解决了 2PC
的单点问题，通过队列解决了 2PC 的同步阻塞问题。

而支持崩溃恢复后数据准确性的就是数据同步了，数据同步基于事务的 ZXID 的唯一性来保证。通过 + 1 操作可以辨别事务的先后顺序。

## 参考文章

本文主要转载自如下，文章内容略有调整：

  * 作者：莫那·鲁道
  * 原文链接：https://www.cnblogs.com/stateis0/p/9062133.html
  * PS：转载请一并附带原出处

其它我还推荐你看下如下相关的较为优秀的文章：

  * [Zookeeper的Leader选举在新窗口打开](https://www.cnblogs.com/leesf456/p/6107600.html)

  * https://blog.csdn.net/a724888/article/details/80757503

  * https://www.cnblogs.com/stateis0/p/9062133.html

 
